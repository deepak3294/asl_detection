{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd26a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from collections import Counter\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "import seaborn as sns          # For heatmap visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f910cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A. CONFIGURATION ---\n",
    "MODEL_PATH = 'asl_cnn_model.h5' # Ensure this points to your trained model\n",
    "IMAGE_SIZE = 32 \n",
    "THRESHOLD_VALUE = 161 # OpenCV threshold (TUNE THIS for your specific lighting)\n",
    "ASL_LETTERS = 'ABCDEFGHIKLMNOPQRSTUVWXY'\n",
    "SESSION_DURATION = 5.0 # Seconds to capture data for each sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d816d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B. MODEL & MEDIAPIPE SETUP ---\n",
    "try:\n",
    "    model = load_model(MODEL_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model '{MODEL_PATH}'. Ensure training was successful.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3819a4a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514d514",
   "metadata": {},
   "source": [
    "--- C. PREPROCESSING FUNCTIONS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b235e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"Processes the cropped hand image to match the model's training input.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.flip(gray, 1)\n",
    "    gray = cv2.GaussianBlur(gray, (15, 15), 0)\n",
    "    \n",
    "    # Using THRESH_BINARY to get a white hand on a black background (if hand is brighter than BG)\n",
    "    # If the hand is darker than BG, change to cv2.THRESH_BINARY_INV\n",
    "    _, thresholded = cv2.threshold(gray, THRESHOLD_VALUE, 255, cv2.THRESH_BINARY) \n",
    "    \n",
    "    resized = cv2.resize(thresholded, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    normalized = resized / 255.0\n",
    "    reshaped = np.reshape(normalized, (1, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    \n",
    "    return reshaped, resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d5e02",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def predict_asl_letter(prediction):\n",
    "    \"\"\"Maps the model's prediction index to the ASL character.\"\"\"\n",
    "    return ASL_LETTERS[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde48e44",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- D. PREDICTION AND EVALUATION LOOP ---\n",
    "def run_predictions(true_label):\n",
    "    \"\"\"Captures predictions for 5 seconds and calculates accuracy for the true_label.\"\"\"\n",
    "    start_time = time.time()\n",
    "    y_true = [] \n",
    "    y_pred = []\n",
    "    \n",
    "    while time.time() - start_time < SESSION_DURATION:\n",
    "        success, frame = cap.read()\n",
    "        if not success: break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(frame_rgb)\n",
    "\n",
    "        if result.multi_hand_landmarks:\n",
    "            \n",
    "            # --- Hand Cropping Logic (Simplified to use the first detected hand) ---\n",
    "            h, w, c = frame.shape\n",
    "            x_min, y_min = w, h\n",
    "            x_max, y_max = 0, 0\n",
    "            \n",
    "            for landmark in result.multi_hand_landmarks[0].landmark:\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                x_min, y_min = min(x_min, x), min(y_min, y)\n",
    "                x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "\n",
    "            margin = 30\n",
    "            x_min = max(0, x_min - margin)\n",
    "            y_min = max(0, y_min - margin)\n",
    "            x_max = min(w, x_max + margin)\n",
    "            y_max = min(h, y_max + margin)\n",
    "\n",
    "            hand_image = frame[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "            if hand_image.size > 0:\n",
    "                preprocessed_image, resized_image = preprocess_image(hand_image)\n",
    "\n",
    "                prediction = model.predict(preprocessed_image, verbose=0)\n",
    "                asl_letter = predict_asl_letter(prediction)\n",
    "                confidence = np.max(prediction) * 100\n",
    "                \n",
    "                # Store data points for metric calculation\n",
    "                y_true.append(true_label)\n",
    "                y_pred.append(asl_letter)\n",
    "\n",
    "                # Draw UI feedback\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'PRED: {asl_letter} ({confidence:.1f}%)', \n",
    "                            (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                cv2.imshow('Preprocessed Image', resized_image)\n",
    "\n",
    "        cv2.putText(frame, f\"HOLD: {true_label} | TIME: {SESSION_DURATION - (time.time() - start_time):.1f}s\", \n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.imshow('ASL Recognition', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- E. MAIN EXECUTION ---\n",
    "cap = cv2.VideoCapture(0)\n",
    "metrics_per_letter = {}\n",
    "y_true_total = [] \n",
    "y_pred_total = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e260b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============================================\")\n",
    "print(\"ASL REAL-TIME EVALUATION MODE\")\n",
    "print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ff126",
   "metadata": {},
   "outputs": [],
   "source": [
    "for true_label in ASL_LETTERS:\n",
    "    print(f\"\\nüëâ Ready for letter: {true_label}. Press 'C' to begin capture.\")\n",
    "    \n",
    "    # Loop to wait for user input (C or Q)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: sys.exit()\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        cv2.putText(frame, f\"Ready: {true_label}. Press 'C' to start 5s test.\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.imshow('ASL Recognition', frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('c'):\n",
    "            print(f\"   Capturing {true_label} for {SESSION_DURATION} seconds.\")\n",
    "            break\n",
    "        elif key == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            sys.exit()\n",
    "\n",
    "    # Run the prediction session\n",
    "    y_true, y_pred = run_predictions(true_label)\n",
    "\n",
    "    # Calculate metrics after prediction session\n",
    "    if y_pred:\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        most_common_label, count = Counter(y_pred).most_common(1)[0]\n",
    "        \n",
    "        metrics_per_letter[true_label] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Most Predicted Label': most_common_label,\n",
    "            'Count': count,\n",
    "            'Total Frames': len(y_pred)\n",
    "        }\n",
    "        \n",
    "        y_true_total.extend(y_true)\n",
    "        y_pred_total.extend(y_pred)\n",
    "        \n",
    "        print(f'   Result: Accuracy: {accuracy:.2f}, Most Predicted: {most_common_label} ({count} frames)')\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    else:\n",
    "        print(\"   No hand detected during the session.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- F. FINAL REPORTING ---\n",
    "if y_true_total:\n",
    "    final_accuracy = accuracy_score(y_true_total, y_pred_total)\n",
    "    \n",
    "    print(\"\\n\\n=============================================\")\n",
    "    print(f\"     FINAL OVERALL REAL-TIME ACCURACY: {final_accuracy:.2f}\")\n",
    "    print(\"=============================================\")\n",
    "\n",
    "    print(\"\\nDetailed Per-Letter Performance:\")\n",
    "    for letter, metrics in metrics_per_letter.items():\n",
    "        print(f'Sign {letter}: ACC={metrics[\"Accuracy\"]:.2f} | PRED={metrics[\"Most Predicted Label\"]} ({metrics[\"Count\"]}/{metrics[\"Total Frames\"]} frames)')\n",
    "    \n",
    "    \n",
    "    # --- G. CONFUSION MATRIX GENERATION AND DISPLAY ---\n",
    "    print(\"\\n--- Generating Confusion Matrix Plot ---\")\n",
    "    \n",
    "    labels_list = list(ASL_LETTERS) \n",
    "    \n",
    "    # Calculate the Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_total, y_pred_total, labels=labels_list)\n",
    "\n",
    "    # Plot the Matrix\n",
    "    plt.figure(figsize=(18, 15)) \n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True,        \n",
    "        fmt='d',           \n",
    "        cmap='Blues',      \n",
    "        xticklabels=labels_list, \n",
    "        yticklabels=labels_list \n",
    "    )\n",
    "    plt.title('Real-Time ASL Detection Confusion Matrix')\n",
    "    plt.ylabel('True Label (Actual Sign Held)')\n",
    "    plt.xlabel('Predicted Label (Model Output)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    print(\"\\nNo full evaluation cycle was completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
