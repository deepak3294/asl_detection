{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11570388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A. CONFIGURATION ---\n",
    "# Initialize MediaPipe hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of letters to capture (excluding J and Z for static ASL)\n",
    "letters = 'TX'\n",
    "NUM_IMAGES_PER_CLASS = 200 # Total images to capture for each letter\n",
    "DATASET_DIR = 'asl_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B. ROI DEFINITION (CRITICAL FOR TRAINING CONSISTENCY) ---\n",
    "# Define the fixed Region of Interest (ROI) boundaries on the right side of the screen\n",
    "# Coordinates (normalized to a 640x480 frame for drawing)\n",
    "ROI_START_X, ROI_START_Y = 400, 50 \n",
    "ROI_END_X, ROI_END_Y = 600, 350\n",
    "ROI_COLOR = (255, 0, 0) # Blue for the ROI box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5c7a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_DIR):\n",
    "    os.makedirs(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e80d70",
   "metadata": {},
   "source": [
    "--- C. CAPTURE FUNCTION ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589cc60",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def capture_images_for_letter(letter, num_images=NUM_IMAGES_PER_CLASS):\n",
    "    letter_dir = os.path.join(DATASET_DIR, letter)\n",
    "    if not os.path.exists(letter_dir):\n",
    "        os.makedirs(letter_dir)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Set standard frame size for consistent ROI drawing\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Ready to capture {letter}. Hold sign and press 'S' ---\")\n",
    "\n",
    "    count = 0\n",
    "    while count < num_images:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        # Flip the frame and get dimensions\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, _ = frame.shape\n",
    "        \n",
    "        # Draw the Fixed ROI Box\n",
    "        cv2.rectangle(frame, (ROI_START_X, ROI_START_Y), (ROI_END_X, ROI_END_Y), ROI_COLOR, 2)\n",
    "        \n",
    "        # Display status\n",
    "        status_text = f\"Class: {letter} | Captured: {count}/{num_images}\"\n",
    "        cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Convert to RGB and process with MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "        \n",
    "        # --- LOGIC TO CHECK ROI AND CROP ---\n",
    "        \n",
    "        hand_detected_in_roi = False\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # 1. Calculate the center of the detected hand\n",
    "                x_center_normalized = sum(lm.x for lm in hand_landmarks.landmark) / len(hand_landmarks.landmark)\n",
    "                y_center_normalized = sum(lm.y for lm in hand_landmarks.landmark) / len(hand_landmarks.landmark)\n",
    "                \n",
    "                x_center = int(x_center_normalized * w)\n",
    "                y_center = int(y_center_normalized * h)\n",
    "                \n",
    "                # 2. Check if the hand center is within the Fixed ROI box\n",
    "                if (ROI_START_X < x_center < ROI_END_X) and (ROI_START_Y < y_center < ROI_END_Y):\n",
    "                    \n",
    "                    # Hand is correctly positioned in the box\n",
    "                    hand_detected_in_roi = True\n",
    "                    \n",
    "                    # Determine the bounding box coordinates (similar to your old logic)\n",
    "                    x_min = int(min(lm.x for lm in hand_landmarks.landmark) * w)\n",
    "                    x_max = int(max(lm.x for lm in hand_landmarks.landmark) * w)\n",
    "                    y_min = int(min(lm.y for lm in hand_landmarks.landmark) * h)\n",
    "                    y_max = int(max(lm.y for lm in hand_landmarks.landmark) * h)\n",
    "                    \n",
    "                    # Apply margin and clip to frame boundaries\n",
    "                    box_margin = 20\n",
    "                    x_min = max(x_min - box_margin, 0)\n",
    "                    x_max = min(x_max + box_margin, w)\n",
    "                    y_min = max(y_min - box_margin, 0)\n",
    "                    y_max = min(y_max + box_margin, h)\n",
    "                    \n",
    "                    # Draw a GREEN box for confirmation\n",
    "                    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Crop the hand region\n",
    "                    hand_region = frame[y_min:y_max, x_min:x_max]\n",
    "                    \n",
    "                    # --- SAVE ACTION ---\n",
    "                    img_path = os.path.join(letter_dir, f'{letter}_{count}_{time.time()}.jpg')\n",
    "                    cv2.imwrite(img_path, hand_region)\n",
    "                    count += 1\n",
    "                    break # Only save one hand per frame\n",
    "\n",
    "        # Draw status message if outside ROI\n",
    "        if not hand_detected_in_roi:\n",
    "            cv2.putText(frame, \"MOVE HAND INTO BLUE BOX\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            \n",
    "        cv2.imshow('ASL Data Collector', frame)\n",
    "\n",
    "        # Break the capture loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Captured {count} images for letter {letter}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c68c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- D. LOOP EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    for letter in letters:\n",
    "        # Before starting capture for the next letter, wait for user confirmation\n",
    "        print(f\"\\nReady for letter {letter}. Press Enter in the terminal to begin capturing.\")\n",
    "        input() # Wait for user input in the console\n",
    "        capture_images_for_letter(letter)\n",
    "\n",
    "    print(\"Dataset creation complete!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
