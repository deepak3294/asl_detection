{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62651d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from keras.models import load_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac6a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A. CONFIGURATION AND HYPERPARAMETERS ---\n",
    "MODEL_PATH = 'asl_cnn_model.h5'\n",
    "IMAGE_SIZE = 32\n",
    "ASL_LETTERS = 'ABCDEFGHIKLMNOPQRSTUVWXY'\n",
    "CONFIDENCE_THRESHOLD = 0.95  # REQUIRED: Only process predictions > 95%\n",
    "APPEND_DELAY_SEC = 2.0       # Time (seconds) sign must be held steady before appending\n",
    "PREDICT_INTERVAL_MS = 50     # Run model prediction only every 50ms (throttling for speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ecd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global tracking variables\n",
    "recognized_text = \"\"\n",
    "previous_letter = \"\"\n",
    "last_append_time = time.time()\n",
    "last_predict_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B. MODEL & MEDIAPIPE SETUP ---\n",
    "try:\n",
    "    model = load_model(MODEL_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c236f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e977bf8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define a function to map the CNN model's prediction to the corresponding ASL letter\n",
    "def predict_asl_letter(prediction_index):\n",
    "    \"\"\"Maps the model's output index to the ASL character.\"\"\"\n",
    "    # Handle bounds check just in case\n",
    "    if 0 <= prediction_index < len(ASL_LETTERS):\n",
    "        return ASL_LETTERS[prediction_index]\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9a9c9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_image(image):\n",
    "    # Convert the hand image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur\n",
    "    gray = cv2.GaussianBlur(gray, (15, 15), 0)\n",
    "    \n",
    "    # Apply binary thresholding (Tuning the 161 value might still be necessary!)\n",
    "    _, thresholded = cv2.threshold(gray, THRESHOLD_VALUE, 255, cv2.THRESH_BINARY) \n",
    "    \n",
    "    # Resize the image to the model's required input size\n",
    "    resized = cv2.resize(thresholded, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    \n",
    "    # Normalize and Reshape (1, 32, 32, 1)\n",
    "    normalized = resized / 255.0\n",
    "    reshaped = np.reshape(normalized, (1, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    \n",
    "    return reshaped, resized # resized is the thresholded image for display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f87a2",
   "metadata": {},
   "source": [
    "--- C. MAIN EXECUTION LOOP ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc527df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Use a static threshold value for the preprocessing function\n",
    "# NOTE: This THRESHOLD_VALUE is not defined globally, so I'm setting a default optimal one here.\n",
    "# You MUST tune this (e.g., to 140 or 161) based on your background.\n",
    "THRESHOLD_VALUE = 161 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success: break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1) # Flip for mirror effect\n",
    "    h, w, c = frame.shape\n",
    "    current_time_ms = cv2.getTickCount() / cv2.getTickFrequency() * 1000 \n",
    "    \n",
    "    predicted_sign = \"\"\n",
    "    confidence = 0.0\n",
    "\n",
    "    # Convert the video frame to RGB for MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    # 1. Prediction Throttling (SPEED FIX)\n",
    "    if (current_time_ms - last_predict_time) > PREDICT_INTERVAL_MS and result.multi_hand_landmarks:\n",
    "        \n",
    "        # Reset prediction time\n",
    "        last_predict_time = current_time_ms\n",
    "        \n",
    "        # --- Hand Cropping Logic ---\n",
    "        hand_landmarks = result.multi_hand_landmarks[0] # Assume only one hand\n",
    "        x_min, y_min = w, h\n",
    "        x_max, y_max = 0, 0\n",
    "        \n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "            x_min, y_min = min(x_min, x), min(y_min, y)\n",
    "            x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "\n",
    "        margin = 30\n",
    "        x_min = max(0, x_min - margin)\n",
    "        y_min = max(0, y_min - margin)\n",
    "        x_max = min(w, x_max + margin)\n",
    "        y_max = min(h, y_max + margin)\n",
    "\n",
    "        hand_image = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        if hand_image.size > 0:\n",
    "            preprocessed_image, resized_image = preprocess_image(hand_image)\n",
    "            \n",
    "            # 2. Use the trained model to predict the ASL letter\n",
    "            prediction = model.predict(preprocessed_image, verbose=0)\n",
    "            predicted_label_index = np.argmax(prediction)\n",
    "            confidence = np.max(prediction)\n",
    "            predicted_sign = predict_asl_letter(predicted_label_index)\n",
    "            \n",
    "            # Store the current highly confident prediction\n",
    "            if confidence > CONFIDENCE_THRESHOLD:\n",
    "                asl_letter = predicted_sign\n",
    "            else:\n",
    "                asl_letter = \"\" # Ignore low confidence predictions\n",
    "            \n",
    "            # Draw bounding box and prediction\n",
    "            box_color = (0, 255, 0) if confidence > CONFIDENCE_THRESHOLD else (0, 165, 255)\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), box_color, 2)\n",
    "            \n",
    "            text_display = f'ASL: {predicted_sign} ({confidence*100:.1f}%)'\n",
    "            cv2.putText(frame, text_display, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, box_color, 2, cv2.LINE_AA)\n",
    "            \n",
    "            cv2.imshow('Preprocessed Image', resized_image)\n",
    "            \n",
    "        else:\n",
    "            asl_letter = \"\" # No image to process\n",
    "    \n",
    "    # 3. Text Appending and Stability Logic (LOGIC FIX)\n",
    "    \n",
    "    # If the current highly confident prediction (asl_letter) matches the previous one\n",
    "    if predicted_sign == previous_letter and predicted_sign != \"\" and confidence >= CONFIDENCE_THRESHOLD:\n",
    "        \n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Append the letter if the required delay has passed\n",
    "        if current_time - last_append_time >= APPEND_DELAY_SEC:\n",
    "            recognized_text += predicted_sign\n",
    "            last_append_time = current_time # Reset timer\n",
    "            \n",
    "    elif predicted_sign != \"\":\n",
    "        # Reset timer and previous letter when a NEW confident sign is seen\n",
    "        previous_letter = predicted_sign\n",
    "        last_append_time = time.time()\n",
    "        \n",
    "    # --- Display Windows ---\n",
    "    cv2.imshow('ASL Recognition', frame)\n",
    "\n",
    "    text_window = np.ones((200, 500, 3), dtype=np.uint8) * 255\n",
    "    cv2.putText(text_window, 'Recognized Text:', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.putText(text_window, recognized_text, (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.imshow('Recognized Text', text_window)\n",
    "\n",
    "    # --- Key Press Controls ---\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'): break\n",
    "    if key == ord('c'): recognized_text = ''\n",
    "    if key == ord('s'): recognized_text += \" \"\n",
    "    if key == ord('b'): recognized_text = recognized_text[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
